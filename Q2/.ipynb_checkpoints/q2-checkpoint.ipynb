{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Photometric Redshifts of Galaxies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1(a) Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand Feature Extraction, it is imperative that we understand what 'feature' means. As I understand it, feature is analogues to 'variable' used in Physics. Thus, a feature is a set of input variables that can be measured independently. In fact, measurement of variables is essential to predict some outcome based on relationships between them. However, there can also be an extention to this definition. New features can be constructed from a combination of other pre-existing features using various mathematical tools.\n",
    "\n",
    "In this sense, we can now begin to define feature extraction. In this context, we have been given a set of data where one column namely the redshift is thought of to be a combination of four other columns which are colours that can be measured. Hence, we term the four colour combinations as features. But, it might turn out that redshift might not depend on combination of all these features i.e some features might be less important or might be dependant on other features or redshift might not be directly related to these features, but in turn is related to an operation done on those features i.e newly constructed features. This entire process of figuring out the 'true variables' is called as Feature Extraction.\n",
    "\n",
    "Note: This is fundamentally different from Feature Selection because Feature Selection assumes that we already know a list of features and hence need to know the best of them, while Feature Extraction doesn't assume anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1(b+c) Photometric Estimator based on Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the data in the votables are transferred to csv files which are easier to handle (see q1 also for more reasons). For this, the python file named vottocsv.py is used. Then we import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import neighbors\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fetch the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Traindata.csv')\n",
    "test = pd.read_csv('Testdata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now select the columns for training the model. We also extract the test data as Ygen and Xgen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[['u-g','g-r','r-i','i-z']]\n",
    "Y = train[['z_spec']]\n",
    "Ygen = test[['z_spec']]\n",
    "Xgen = test[['u-g','g-r','r-i','i-z']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting a linear algorithm using the entire train dataset will cause an overfit of the data by the algorithm to a very strong extent. Hence, instead we need to train this dataset by using chunks of the first dataset to first train the algorithm and then try to fit it with another chunk of the same dataset to reduce the overall overfitting. However, the model still remains biased to this dataset.\n",
    "\n",
    "For tuning the model, we now divide the train data into sets of 'train' and 'test' datasets using k-fold cross validation. The code is inspired from the code used in the assignments. Then we use a Linear Regression model to fit the train data sets and predict values for the test data sets. This then gives us the 'Training Error' for the first data set. However, this is not the true error. Since, we use exclusively one dataset to train our algorithm, the algorithmn might be biased towards fitting the first set of data only. Hence, a second set of data taken at a different time or place can say us how well this model actually works. This is done by applying the regressor to Xgen from the second dataset and comparing it with Ygen. This error is called 'Generalization Error'. The training error is shown first followed by the generalization error which is obatained by testing the trained model on the second dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TRAIN:', array([14862, 14863, 14864, ..., 74306, 74307, 74308]), 'TEST:', array([    0,     1,     2, ..., 14859, 14860, 14861]))\n",
      "(array([-0.22289139]), array([[-0.02558786,  0.11659748,  0.67506641,  0.00336796]]), 0.77273877318220374)\n",
      "Training Error is 0.0152035390976\n",
      "Generalization Error is 0.015156971496\n",
      "('TRAIN:', array([    0,     1,     2, ..., 74306, 74307, 74308]), 'TEST:', array([14862, 14863, 14864, ..., 29721, 29722, 29723]))\n",
      "(array([-0.22042711]), array([[-0.02642282,  0.11579633,  0.67301541,  0.00735718]]), 0.76690562554573694)\n",
      "Training Error is 0.0144620329545\n",
      "Generalization Error is 0.0151699789005\n",
      "('TRAIN:', array([    0,     1,     2, ..., 74306, 74307, 74308]), 'TEST:', array([29724, 29725, 29726, ..., 44583, 44584, 44585]))\n",
      "(array([-0.22220995]), array([[-0.02592871,  0.11580501,  0.67458606,  0.0073301 ]]), 0.76990746597627346)\n",
      "Training Error is 0.0147754956634\n",
      "Generalization Error is 0.0151676828116\n",
      "('TRAIN:', array([    0,     1,     2, ..., 74306, 74307, 74308]), 'TEST:', array([44586, 44587, 44588, ..., 59445, 59446, 59447]))\n",
      "(array([-0.21808083]), array([[-0.0272374 ,  0.11506924,  0.67060121,  0.01282819]]), 0.76640486338181713)\n",
      "Training Error is 0.0147983112779\n",
      "Generalization Error is 0.0152020021571\n",
      "('TRAIN:', array([    0,     1,     2, ..., 59445, 59446, 59447]), 'TEST:', array([59448, 59449, 59450, ..., 74306, 74307, 74308]))\n",
      "(array([-0.2062866]), array([[-0.02963605,  0.10984472,  0.67782757,  0.0065734 ]]), 0.77259269214719928)\n",
      "Training Error is 0.0166327438371\n",
      "Generalization Error is 0.0154874267482\n"
     ]
    }
   ],
   "source": [
    "k = KFold(n_splits = 5)\n",
    "\n",
    "for train_index, test_index in k.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index] #.iloc is added to take care of indices to dataframe incompatibility\n",
    "    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    l = LinearRegression()\n",
    "    l.fit(X_train, Y_train) \n",
    "    print(l.intercept_, l.coef_, l.score(X_train,Y_train)) # Parameters of the fit\n",
    "    t = (Y_test-l.predict(X_test))/(1+Y_test)\n",
    "    t1 = t.abs()\n",
    "    Error = t1.median()\n",
    "    print  'Training Error is', Error['z_spec'] # The training error\n",
    "    g = (Ygen-l.predict(Xgen))/(1+Ygen)\n",
    "    g1 = g.abs()\n",
    "    Errorg = g1.median()\n",
    "    print  'Generalization Error is', Errorg['z_spec'] # The generalization error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easily seen that the train and test error are comparable and the parameters for the model are not large. Thus, we don't need a regularization technique like ridge or lasso. It also means the training has been done well and the data actually follows a linear trend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1(d) Non-linear Photometric estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have tried out a Linear Estimator, let's use a non-linear estimator of our choice. I use k-Nearest Neighbours regressor. I personally favour this because I understand it more clearly among all non-linear methods. However, there is also another reason to use this. The features we consider have only 4 dimensions which isn't much and all features can very well be assumed to be continuous. Knn is very effective in these situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For knn I take a high number of neighbours for a good averaging and I weigh all of them by distance, so that the closer points get more prefrence. This is partly inspired by the fact that I already know that the data trend is fairly linear but still has a significant amount of variation from a linear trend. This is known from seeing the linear scores (l.score(X,Y) computed in the linear model iterations with almost all of them being close to a value of 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TRAIN:', array([14862, 14863, 14864, ..., 74306, 74307, 74308]), 'TEST:', array([    0,     1,     2, ..., 14859, 14860, 14861]))\n",
      "Training Error is 0.0127386609135\n",
      "Generalization Error is 0.0127799120494\n",
      "('TRAIN:', array([    0,     1,     2, ..., 74306, 74307, 74308]), 'TEST:', array([14862, 14863, 14864, ..., 29721, 29722, 29723]))\n",
      "Training Error is 0.0120293279604\n",
      "Generalization Error is 0.0127819611231\n",
      "('TRAIN:', array([    0,     1,     2, ..., 74306, 74307, 74308]), 'TEST:', array([29724, 29725, 29726, ..., 44583, 44584, 44585]))\n",
      "Training Error is 0.0124790116275\n",
      "Generalization Error is 0.0127756396825\n",
      "('TRAIN:', array([    0,     1,     2, ..., 74306, 74307, 74308]), 'TEST:', array([44586, 44587, 44588, ..., 59445, 59446, 59447]))\n",
      "Training Error is 0.0122587697669\n",
      "Generalization Error is 0.01283689562\n",
      "('TRAIN:', array([    0,     1,     2, ..., 59445, 59446, 59447]), 'TEST:', array([59448, 59449, 59450, ..., 74306, 74307, 74308]))\n",
      "Training Error is 0.0138375971945\n",
      "Generalization Error is 0.0127997917148\n"
     ]
    }
   ],
   "source": [
    "k = KFold(n_splits = 5)\n",
    "\n",
    "for train_index, test_index in k.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index] #.iloc is added to take care of indices to dataframe incompatibility\n",
    "    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    knn = neighbors.KNeighborsRegressor(n_neighbors = 10, weights = 'distance') \n",
    "    knn.fit(X_train, Y_train)\n",
    "    t = (Y_test-knn.predict(X_test))/(1.0+Y_test)\n",
    "    t1 = t.abs()\n",
    "    Error = t1.median()\n",
    "    print 'Training Error is', Error['z_spec'] # Training Error\n",
    "    gk = (Ygen-knn.predict(Xgen))/(1+Ygen)\n",
    "    gk1 = gk.abs()\n",
    "    Errorgk = gk1.median()\n",
    "    print  'Generalization Error is', Errorgk['z_spec'] # Generalization Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is seen that the training errors are considerably lesser than values with a linear model. Not only that, the generalization errors show that the non-linear model also holds up very well. The generalization errors obtained here are less than the generalization errors obtained for the linear case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1(e) Advantages and Disadvantages of KNN over Linear Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. KNN estimator is very flexible as it is non-parametric. So, almost all values in the training set are considered for fitting. This can also be a disadvantage as we will see later.\n",
    "2. There are only 4 features and the data is fairly linear, so careful use of the non-parametric feature will better constrain the model while training. In fact, training by KNN is actually more effective here. If we would have fit the entire train file without weighting and with less neighbours, the training error will be very low (See Appendix). But, it will overfit the data to a very large amount, so the generalization error is larger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The flexibilty of the approach also means that it is prone to interference from outlier data. This would mean that we would be in a sense overfitting the data if we don't take considerable care in selecting the number of neighbours and the weighting.\n",
    "2. The KNN estimator is also computationally more intensive. It searches for neighbours and gives weights by calculating the distance. This takes longer the more features there are. Fortunately, we have only 4 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN using the whole train set and then applied to the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error is 0.0\n",
      "Generalization Error is 0.0127373528291\n"
     ]
    }
   ],
   "source": [
    "knn = neighbors.KNeighborsRegressor(n_neighbors = 10, weights = 'distance') \n",
    "knn.fit(X, Y)\n",
    "ty = (Y-knn.predict(X))/(1.0+Y)\n",
    "ty1 = ty.abs()\n",
    "Errory = ty1.median()\n",
    "print 'Training Error is', Errory['z_spec'] # Training Error\n",
    "gyk = (Ygen-knn.predict(Xgen))/(1+Ygen)\n",
    "gyk1 = gyk.abs()\n",
    "Errorgyk = gyk1.median()\n",
    "print  'Generalization Error is', Errorgyk['z_spec'] # Generalization Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to see that the data fits the training data almost perfectly but it doesn't do the same for the test dataset. This is because it overfits the train data, which is exactly what we don't want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The material uses the scikit-learn software package version 0.19.1. I also took help from the online documentation of the packages used to write my code. In addition, code from the assignment was also liberally used. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
